{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get text and duration dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from navec import Navec\n",
    "from slovnet.model.emb import NavecEmbedding\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>я слушаю</td>\n",
       "      <td>5.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>каким стал сбер</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>где родился шерлок холмс</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>открой в браузере ennio morricone</td>\n",
       "      <td>8.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>каким стал сбер</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        speaker_text  duration\n",
       "0                           я слушаю      5.82\n",
       "1                    каким стал сбер      3.70\n",
       "2           где родился шерлок холмс      4.38\n",
       "3  открой в браузере ennio morricone      8.58\n",
       "4                    каким стал сбер      3.70"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/viktor/Projects/Data/MagistracyDeploma/'\n",
    "test_path = data_path + 'crowd_test/'\n",
    "raw_crowd_test_path = test_path + 'raw_crowd_test.tsv'\n",
    "raw_crowd_test = pd.read_csv(raw_crowd_test_path ,sep='\\t')\n",
    "text_column_name = 'speaker_text'\n",
    "duration_column_name = 'duration'\n",
    "duration_text = raw_crowd_test[[text_column_name, duration_column_name]]\n",
    "# duration_text.info()\n",
    "duration_text = duration_text.dropna()\n",
    "duration_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77833, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/Projects/Univer/Deploma/.venv/lib/python3.11/site-packages/slovnet/model/emb.py:46: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  torch.from_numpy(navec.pq.indexes),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 300])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = '/home/viktor/Projects/Data/MagistracyDeploma/TestHypotheses/'\n",
    "file_name = 'navec_hudlit_v1_12B_500K_300d_100q.tar'  # 51MB\n",
    "path = os.path.join(data_folder, file_name)\n",
    "\n",
    "navec = Navec.load(path)  # ~1 sec, ~100MB RAM\n",
    "\n",
    "words = ['навек', '<unk>', '<pad>']\n",
    "ids = [navec.vocab[_] for _ in words]\n",
    "\n",
    "emb = NavecEmbedding(navec)\n",
    "vocab = navec.vocab\n",
    "input = torch.tensor(ids)\n",
    "\n",
    "emb(input).shape  # 3 x 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4161, -0.8234,  0.1041,  0.2171, -0.1972, -0.2072,  0.4124,  0.3353,\n",
       "         0.4763, -0.1746, -0.3656,  0.6460, -0.2359, -0.4967,  0.2351,  0.0293,\n",
       "         0.5579, -0.1821,  0.3460, -0.4691, -0.1072, -0.2880, -0.1078,  0.0996,\n",
       "        -0.4523,  0.4564,  0.6884, -0.1146, -0.0627, -0.2884,  0.3378, -0.2925,\n",
       "         0.5172,  0.8344, -0.2078, -0.0337, -0.0421,  0.2375,  0.3722,  0.1258,\n",
       "         0.1039, -0.2675, -0.1411,  0.1203, -0.4903, -0.0273, -0.0957, -0.0155,\n",
       "         0.3530, -0.0187, -0.2534,  0.0073, -0.1871,  0.2632,  0.0475, -0.4049,\n",
       "        -0.0334, -0.0777,  0.2896,  0.1553, -0.1509, -0.3095,  0.1722, -0.1822,\n",
       "        -0.0854, -0.1743,  0.2572,  0.0155,  0.3648,  0.0846,  0.1715, -0.3526,\n",
       "         0.7443,  0.0164,  0.5548, -0.1972,  0.3350, -0.0060, -0.2826,  0.1762,\n",
       "        -0.2713,  0.0669, -0.3925,  0.2355,  0.2635,  0.2953, -0.6040,  0.1477,\n",
       "         0.0168, -0.6317,  0.0766, -0.4718, -0.2756,  0.3199, -0.2244, -0.0384,\n",
       "         0.2538, -0.1047,  0.2621,  0.0517,  0.2152, -0.0406, -0.0966, -0.0651,\n",
       "         0.2556,  0.2588, -0.0987,  0.2605, -0.1365,  0.3386, -0.2634, -0.6456,\n",
       "         0.0287, -0.1442, -0.1452, -0.6891, -0.5814,  0.3150, -0.1645, -0.1749,\n",
       "         0.1888,  0.0362, -0.1685,  0.1747,  0.4983, -0.0147,  0.0715, -0.2776,\n",
       "        -0.2506, -0.2743,  0.5686, -0.2263, -0.6408, -0.2925, -0.3039, -0.2757,\n",
       "        -0.0464,  0.6064,  0.3105,  0.0033, -0.0557,  0.0864,  0.1653, -0.3429,\n",
       "        -0.3416,  0.1474, -0.0156, -0.5646,  0.1434,  0.2449,  0.0398, -0.1010,\n",
       "        -0.0413, -0.3544, -0.3592, -0.1576,  0.0283, -0.3621,  0.1957,  0.1655,\n",
       "        -0.3025, -0.5526, -0.2441,  0.1166, -0.2381,  0.2096, -0.5578, -0.2109,\n",
       "         0.4997, -0.0738,  0.0306,  0.3745, -0.0982, -0.1119,  0.7998,  0.2025,\n",
       "         0.2922,  0.3499,  0.5105,  0.1876,  0.0599, -0.0821, -0.0710,  0.0489,\n",
       "         0.0979, -0.4666, -0.4398, -0.1099, -0.4004,  0.0857, -0.8822,  0.0297,\n",
       "         0.6507,  0.2814,  0.0082, -0.5602, -0.1433,  0.4211,  0.1256,  0.6501,\n",
       "         0.0888, -0.0875, -0.3383,  0.0271,  0.0471, -0.6374, -0.2041, -0.3043,\n",
       "         0.0663, -0.3767,  0.1702,  0.5663, -0.4121, -0.6302,  0.5167,  0.1974,\n",
       "         0.3545,  0.3506,  0.1205, -0.0186,  0.0199, -0.0800,  0.3271, -0.6576,\n",
       "        -0.0031,  0.1327,  0.2685, -0.1143,  0.0928, -0.2763,  0.1367, -0.4326,\n",
       "        -0.3913,  0.0912,  0.3211,  0.1353,  0.1747, -0.2239,  0.1326,  0.1367,\n",
       "        -0.3565,  0.0729,  0.3847, -0.3405, -0.0575, -0.2698,  0.2795, -0.4320,\n",
       "         0.0799, -0.1480,  0.4046, -0.1025, -0.1756,  0.1252,  0.5078, -0.3781,\n",
       "         0.4742,  0.1323, -0.1159,  0.3491, -0.0075, -0.4531, -0.3940,  0.2603,\n",
       "         0.0203, -0.6541,  0.1180, -0.5008, -0.2822,  0.1689,  0.0531,  0.0786,\n",
       "        -0.1139, -0.2955, -0.7207,  0.6272, -0.2904, -0.1264,  0.1896, -0.0135,\n",
       "         0.3455,  0.2015,  0.0726,  0.4840, -0.1560, -0.2748,  0.0077, -0.4727,\n",
       "         0.1518, -0.3055, -0.8119, -0.4393,  0.1407,  0.1485,  0.4692,  0.1279,\n",
       "         0.4451, -0.2127, -0.4413, -0.0091])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emb(torch.tensor(navec.vocab['яма']))\n",
    "def word_to_emb(emb, vocab, word_or_words: Union[str, list[str]]) -> torch.Tensor:\n",
    "    def single_word_to_emb(emb, vocab, word):\n",
    "        return emb(torch.tensor(vocab[word]))\n",
    "    if type(word_or_words) == list:\n",
    "        result = []\n",
    "        for word in word_or_words:\n",
    "            try:\n",
    "                word_emb = single_word_to_emb(emb, vocab, word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            result.append(word_emb)\n",
    "        return result\n",
    "    else:\n",
    "        return single_word_to_emb(emb, vocab, word_or_words)\n",
    "\n",
    "example_word = 'кот'\n",
    "default_embedding_size = word_to_emb(emb, vocab, example_word).shape\n",
    "word_to_emb(emb, vocab, example_word)\n",
    "# word_to_emb(emb, vocab, ['витя', 'ваня'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bag_of_words(emb, vocab, words: list[str], embedding_size = default_embedding_size) -> torch.Tensor:\n",
    "    word_embeddings = word_to_emb(emb, vocab, words)\n",
    "    vector_sum = torch.zeros(embedding_size)\n",
    "    # if len(word_embeddings) > 0:\n",
    "    for word in word_embeddings:\n",
    "        vector_sum += word\n",
    "    \n",
    "    return vector_sum / len(word_embeddings)\n",
    "\n",
    "bag_of_words(emb, vocab, ['кот', 'телефон']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bags_of_words = []\n",
    "for text in duration_text['speaker_text']:\n",
    "    words = text.lower().split(' ')\n",
    "    text_bags_of_words.append(bag_of_words(emb, vocab, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(text_bags_of_words[0])\n",
    "# text_bags_of_words = torch.stack(text_bags_of_words)\n",
    "# text_bags_of_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_bags_of_words = pd.DataFrame.from_records(text_bags_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/Projects/Univer/Deploma/.venv/lib/python3.11/site-packages/pandas/core/construction.py:814: RuntimeWarning: invalid value encountered in cast\n",
      "  subarr = np.array(arr, dtype=dtype, copy=copy)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.197958</td>\n",
       "      <td>-0.149894</td>\n",
       "      <td>0.032033</td>\n",
       "      <td>-0.102101</td>\n",
       "      <td>-0.219139</td>\n",
       "      <td>-0.263596</td>\n",
       "      <td>-0.225147</td>\n",
       "      <td>0.258930</td>\n",
       "      <td>0.117051</td>\n",
       "      <td>0.100377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068739</td>\n",
       "      <td>0.077948</td>\n",
       "      <td>-0.084094</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>-0.028043</td>\n",
       "      <td>-0.415415</td>\n",
       "      <td>0.147565</td>\n",
       "      <td>-0.205027</td>\n",
       "      <td>-0.019849</td>\n",
       "      <td>0.055699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062094</td>\n",
       "      <td>-0.474676</td>\n",
       "      <td>-0.409619</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>-0.104335</td>\n",
       "      <td>-0.269172</td>\n",
       "      <td>-0.068685</td>\n",
       "      <td>0.292174</td>\n",
       "      <td>-0.271988</td>\n",
       "      <td>-0.157699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213001</td>\n",
       "      <td>-0.233934</td>\n",
       "      <td>-0.219408</td>\n",
       "      <td>-0.407954</td>\n",
       "      <td>0.156611</td>\n",
       "      <td>-0.036210</td>\n",
       "      <td>-0.002245</td>\n",
       "      <td>-0.142691</td>\n",
       "      <td>-0.161408</td>\n",
       "      <td>0.285355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.199201</td>\n",
       "      <td>-0.201970</td>\n",
       "      <td>-0.107651</td>\n",
       "      <td>0.102246</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>-0.182565</td>\n",
       "      <td>-0.134217</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0.119740</td>\n",
       "      <td>-0.133304</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142165</td>\n",
       "      <td>-0.002077</td>\n",
       "      <td>-0.073691</td>\n",
       "      <td>-0.169712</td>\n",
       "      <td>0.187998</td>\n",
       "      <td>-0.286952</td>\n",
       "      <td>0.018901</td>\n",
       "      <td>-0.215588</td>\n",
       "      <td>-0.194586</td>\n",
       "      <td>-0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136984</td>\n",
       "      <td>-0.363209</td>\n",
       "      <td>-0.340083</td>\n",
       "      <td>0.231929</td>\n",
       "      <td>-0.179154</td>\n",
       "      <td>0.081780</td>\n",
       "      <td>0.023477</td>\n",
       "      <td>0.386594</td>\n",
       "      <td>0.389532</td>\n",
       "      <td>-0.062063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294220</td>\n",
       "      <td>0.183978</td>\n",
       "      <td>0.257248</td>\n",
       "      <td>-0.128505</td>\n",
       "      <td>0.063109</td>\n",
       "      <td>-0.345005</td>\n",
       "      <td>0.086596</td>\n",
       "      <td>0.171087</td>\n",
       "      <td>0.158903</td>\n",
       "      <td>-0.257887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062094</td>\n",
       "      <td>-0.474676</td>\n",
       "      <td>-0.409619</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>-0.104335</td>\n",
       "      <td>-0.269172</td>\n",
       "      <td>-0.068685</td>\n",
       "      <td>0.292174</td>\n",
       "      <td>-0.271988</td>\n",
       "      <td>-0.157699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213001</td>\n",
       "      <td>-0.233934</td>\n",
       "      <td>-0.219408</td>\n",
       "      <td>-0.407954</td>\n",
       "      <td>0.156611</td>\n",
       "      <td>-0.036210</td>\n",
       "      <td>-0.002245</td>\n",
       "      <td>-0.142691</td>\n",
       "      <td>-0.161408</td>\n",
       "      <td>0.285355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.197958 -0.149894  0.032033 -0.102101 -0.219139 -0.263596 -0.225147   \n",
       "1  0.062094 -0.474676 -0.409619  0.015491 -0.104335 -0.269172 -0.068685   \n",
       "2 -0.199201 -0.201970 -0.107651  0.102246  0.001738 -0.182565 -0.134217   \n",
       "3  0.136984 -0.363209 -0.340083  0.231929 -0.179154  0.081780  0.023477   \n",
       "4  0.062094 -0.474676 -0.409619  0.015491 -0.104335 -0.269172 -0.068685   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0  0.258930  0.117051  0.100377  ... -0.068739  0.077948 -0.084094  0.113900   \n",
       "1  0.292174 -0.271988 -0.157699  ... -0.213001 -0.233934 -0.219408 -0.407954   \n",
       "2 -0.035429  0.119740 -0.133304  ... -0.142165 -0.002077 -0.073691 -0.169712   \n",
       "3  0.386594  0.389532 -0.062063  ... -0.294220  0.183978  0.257248 -0.128505   \n",
       "4  0.292174 -0.271988 -0.157699  ... -0.213001 -0.233934 -0.219408 -0.407954   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0 -0.028043 -0.415415  0.147565 -0.205027 -0.019849  0.055699  \n",
       "1  0.156611 -0.036210 -0.002245 -0.142691 -0.161408  0.285355  \n",
       "2  0.187998 -0.286952  0.018901 -0.215588 -0.194586 -0.000220  \n",
       "3  0.063109 -0.345005  0.086596  0.171087  0.158903 -0.257887  \n",
       "4  0.156611 -0.036210 -0.002245 -0.142691 -0.161408  0.285355  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_bags_of_words_df = pd.DataFrame(text_bags_of_words, dtype='float')\n",
    "text_bags_of_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77833, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_bags_of_words_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
