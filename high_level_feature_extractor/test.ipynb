{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import wave\n",
    "from scipy.fft import rfft, irfft\n",
    "import sys\n",
    "import soundfile as sf\n",
    "import pyloudnorm as pyln\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "\n",
    "sys.path.append('..')\n",
    "from audio import Audio\n",
    "from volume.human_speech import (\n",
    "    HUMAN_SPEECH_FREQ_BOTTOM,\n",
    "    HUMAN_SPEECH_FREQ_TOP,\n",
    "    HIGH_FREQUENCY_SPEECH_THRESHOLD,\n",
    ")\n",
    "from configs.base import (\n",
    "    RB_FILE_READING_MODE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_AUDIO_PATH:Path = Path('/data/vkiselev/data/other/univer/deploma/dusha/crowd/crowd_train/wavs/00000d522439136554c888f4cfd92131.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Audio(sample_width=2, sr=16000, n_frames=85120, data=array([  0,   0,   0, ..., -10,   1,  -1], dtype=int16), n_channels=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_example:Audio = Audio.wav_file_path_init(path=EXAMPLE_AUDIO_PATH)\n",
    "audio_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Audio(sample_width=2, sr=16000, n_frames=85120, data=array([ 1,  1,  1, ..., -4,  2,  0], dtype=int16), n_channels=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def speech_filter(\n",
    "    audio:Audio, \n",
    "    low_freq=HUMAN_SPEECH_FREQ_BOTTOM, \n",
    "    high_freq=HUMAN_SPEECH_FREQ_TOP,\n",
    "    )->Audio:\n",
    "\n",
    "    fft_result:np.ndarray = rfft(audio.data)\n",
    "    fft_result_filtered:np.ndarray = fft_result.copy()\n",
    "    freqs:np.ndarray = np.fft.fftfreq(audio.n_frames, d=1.0/audio.sr)\n",
    "\n",
    "    positive_freqs:np.ndarray = freqs[:len(freqs) // 2 + 1]\n",
    "\n",
    "    for i, freq in enumerate(positive_freqs):\n",
    "        if abs(freq) > high_freq or abs(freq) < low_freq:\n",
    "            fft_result_filtered[i] = 0\n",
    "\n",
    "    filtered_signal:np.ndarray = irfft(fft_result_filtered)\n",
    "    sample_dtype:type = audio.sample_dtype()\n",
    "    filtered_signal:np.ndarray = filtered_signal.astype(sample_dtype) \n",
    "    return audio.new_data_copy(data=filtered_signal)\n",
    "\n",
    "audio_filtered = speech_filter(audio=audio_example)\n",
    "\n",
    "speech_filter(audio=audio_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_volume(\n",
    "    audio_path:Path,\n",
    "    )->np.float64:\n",
    "    try:\n",
    "        data, rate = sf.read(audio_path)\n",
    "        meter:pyln.meter.Meter = pyln.Meter(rate)\n",
    "        return meter.integrated_loudness(data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "loudness = audio_volume(EXAMPLE_AUDIO_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, int)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, rate = sf.read(EXAMPLE_AUDIO_PATH)\n",
    "type(data), type(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.024670111756333636)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wav_path_2_frequency_features(\n",
    "    file_path:Path,\n",
    "    )->np.float64:\n",
    "    sampling_rate, signal = wavfile.read(file_path)\n",
    "    # Normalize to [-1, 1]\n",
    "    signal:np.ndarray = signal / np.max(np.abs(signal))\n",
    "\n",
    "    # Apply Hann window\n",
    "    window:np.ndarray = np.hanning(len(signal))\n",
    "    signal_windowed:np.ndarray = signal * window\n",
    "\n",
    "    n:int = len(signal_windowed)\n",
    "    freq_magnitudes:np.ndarray = np.abs(np.fft.fft(signal_windowed))\n",
    "    freqs:np.ndarray = np.fft.fftfreq(n, d=1/sampling_rate)\n",
    "\n",
    "    # Keep only positive frequencies (half the spectrum)\n",
    "    positive_freqs:np.ndarray = freqs[:n//2]\n",
    "    positive_magnitudes:np.ndarray = freq_magnitudes[:n//2]\n",
    "\n",
    "    # Convert magnitudes to power (energy)\n",
    "    power_spectrum:np.ndarray = positive_magnitudes ** 2\n",
    "\n",
    "    total_energy:np.float64 = np.sum(power_spectrum)\n",
    "    high_freq_mask:np.ndarray = positive_freqs > HIGH_FREQUENCY_SPEECH_THRESHOLD  # Adjust threshold as needed\n",
    "    high_freq_energy:np.float64 = np.sum(power_spectrum[high_freq_mask])\n",
    "\n",
    "    ratio:np.float64 = high_freq_energy / total_energy\n",
    "    return ratio \n",
    "\n",
    "wav_path_2_frequency_features(EXAMPLE_AUDIO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-frequency ratio: 0.0247\n"
     ]
    }
   ],
   "source": [
    "ratio = high_freq_energy / total_energy\n",
    "print(f\"High-frequency ratio: {ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 85 Hz and 8 kHz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
