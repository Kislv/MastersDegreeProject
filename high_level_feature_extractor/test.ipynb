{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import wave\n",
    "import sys\n",
    "import soundfile as sf\n",
    "# from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "from dataclasses import (\n",
    "    dataclass,\n",
    "    asdict,\n",
    "    fields,\n",
    ")\n",
    "from typing import (\n",
    "    Optional,\n",
    "    Callable,\n",
    "    Set,\n",
    "    Generator,\n",
    "    List,\n",
    "    Tuple,\n",
    "    Union,\n",
    "    Dict,\n",
    ")\n",
    "from functools import (\n",
    "    partial,\n",
    ")\n",
    "import time\n",
    "import dill\n",
    "import logging\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "import torch\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from enum import Enum\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from bdw.check import Check\n",
    "\n",
    "sys.path.append('..')\n",
    "from audio import (\n",
    "    Audio,\n",
    "    WAVFilePathInitArgs,\n",
    ")\n",
    "from text.profanity import (\n",
    "    PROFANITY_WORD_FILTER_LANG_NAME,\n",
    ")\n",
    "from configs.base import (\n",
    "    RB_OPEN_FILE_MODE,\n",
    "    SECONDS_QUANTITY_IN_MINUTE,\n",
    "    TAB,\n",
    "    RUSSIAN_VOWELS,\n",
    "    WB_OPEN_FILE_MODE,\n",
    "    DROP_DUPLICATES_KEEP_FIRST,\n",
    "    JOIN_HOW_INNER,\n",
    "    AGGREGATED_KEYWORD,\n",
    ")\n",
    "from configs.paths import (\n",
    "    DUSHA_CROWD_TRAIN_FILE_PATH,\n",
    "    DUSHA_CROWD_TEST_FILE_PATH,\n",
    "    DUSHA_CROWD_TRAIN_WAVS_DIR_PATH,\n",
    "    DUSHA_CROWD_TEST_WAVS_DIR_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TRAIN_HLF_LAST_VERSION_FILE_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TEST_HLF_LAST_VERSION_FILE_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TRAIN_HLF_STABLE_VERSION_FILE_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TEST_HLF_STABLE_VERSION_FILE_PATH,\n",
    "    DO_NOT_EXTRACTED_FEATUERS_HASHES_FILE_PATH,\n",
    "    PROCESSED_DUSHA_SER_TABLES_TRAIN_FILE_PATH,\n",
    "    PROCESSED_DUSHA_SER_TABLES_TEST_FILE_PATH,\n",
    ")\n",
    "from configs.report_tables_format import (\n",
    "    classification_report_formatted,\n",
    ")\n",
    "from processing.text.normalization import (\n",
    "    normalized_tokens_2_normalized_text,\n",
    "    text_2_normalized_text,\n",
    ")\n",
    "from high_level_feature_extractor.text.profanity import (\n",
    "    text_2_is_contain_swear_words,\n",
    ")\n",
    "from high_level_feature_extractor.text.all import (\n",
    "    TranscriptionHighLevelFeatures,\n",
    ")\n",
    "from high_level_feature_extractor.extractor import (\n",
    "    HighLevelSpeechFeatures,\n",
    "    HashHLF,\n",
    "    hash_HLF_list_2_df,\n",
    "    PronounceSpeed,\n",
    ")\n",
    "from high_level_feature_extractor.extract import (\n",
    "    raw_crowd_2_HLF,\n",
    ")\n",
    "from utils.dataclass import (\n",
    "    flatten_dict,\n",
    ")\n",
    "from volume.human_speech import (\n",
    "    HIGH_FREQUENCY_SPEECH_THRESHOLD,\n",
    ")\n",
    "from configs.paths import (\n",
    "    PROCESSED_DUSHA_CROWD_TRAIN_DIR_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TEST_DIR_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TRAIN_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TEST_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH,\n",
    ")\n",
    "from models.config import (\n",
    "    TORCH_TENSORS_KEYWOED,\n",
    "    ATTENTION_MASK_KEYWORD,\n",
    ")\n",
    "from models.text_embedding.ru_en_RoSBERTa import (\n",
    "    DEVICE as ROSBERTA_DEVICE,\n",
    "    NORMALIZE_P as ROSBERTA_NORMALIZE_P,\n",
    "    NORMALIZE_DIM as ROSBERTA_NORMALIZE_DIM,\n",
    "    CLAMP_MIN,\n",
    ")\n",
    "from config import (\n",
    "    SPEAKER_TEXT_FIELD_NAME,\n",
    ")\n",
    "from utils.parallel_processing import (\n",
    "    divide_into_chunks,\n",
    ")\n",
    "from configs.datasets.dusha import (\n",
    "    HASH_ID_COLUMN_NAME,\n",
    "    GoldenEmo,\n",
    "    SPEAKER_EMOTION_FIELD_NAME,\n",
    "    ANNOTATOR_EMOION_FIELD_NAME,\n",
    "    ANNOTATOR_ANSWERS_AGGREGATING_THRESHOLD,\n",
    "    ANNOTATOR_AGGREGATED_FIELD_NAME,\n",
    "    aggregate_crowd,\n",
    ")\n",
    "from processing.text.normalization import (\n",
    "    text_to_normalized_tokens,\n",
    ")\n",
    "from configs.datasets.aggregation import (\n",
    "    aggregate_by_mode,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_AUDIO_PATH:Path = Path('/data01/vvkiselev/data/other/dpl/dusha/crowd/crowd_train/wavs/000039c2bc753aa5a776621a4707eb73.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Audio(hash='000039c2bc753aa5a776621a4707eb73', sample_width=2, sr=16000, n_frames=165120, data=array([ 0,  0,  0, ..., -2,  6, -9], dtype=int16), n_channels=1, _transcription='ахах, пиздец')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio_example:Audio = Audio.wav_file_path_init(path=EXAMPLE_AUDIO_PATH, transcription='ахах, пиздец')\n",
    "arguments:WAVFilePathInitArgs = WAVFilePathInitArgs(path=EXAMPLE_AUDIO_PATH, transcription='ахах, пиздец')\n",
    "audio_example:Audio = Audio.wav_file_path_init(arguments=arguments)\n",
    "audio_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(tfidf_matrix) = <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HighLevelSpeechFeatures(loudness=59.77829826935232, HF_power_ratio=0.054469042401762625, pronounce_speed=PronounceSpeed(WPS=1.065891472868217, LPS=0.9689922480620154, SPS=0.38759689922480617), transcription_features=TranscriptionHighLevelFeatures(mean_words_length=5.0, profanity_words_ratio=0.5, meaning=4.454233000760066e-05, POS_ratio=POS_ratio(ADVB=0.0, COMP=0.0, CONJ=0.0, GRND=0.0, INFN=0.0, INTJ=0.0, PRCL=0.0, PRED=0.0, PREP=0.0, VERB=0.0, ADJS=0.0, PRTS=0.0, NOUN=1.0, ADJF=0.0, NUMR=0.0, PRTF=0.0, NONE=0.0)))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HLF_example:HighLevelSpeechFeatures = HighLevelSpeechFeatures.wav_path_init(path=EXAMPLE_AUDIO_PATH, transcription='бля зачем')\n",
    "HLF_example:HighLevelSpeechFeatures = HighLevelSpeechFeatures.audio_init(audio=audio_example)\n",
    "HLF_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(906953, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_id</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>duration</th>\n",
       "      <th>annotator_emo</th>\n",
       "      <th>golden_emo</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>speaker_emo</th>\n",
       "      <th>source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475e76f77ac1ed7cabafca740b15b32a</td>\n",
       "      <td>wavs/475e76f77ac1ed7cabafca740b15b32a.wav</td>\n",
       "      <td>2.453000</td>\n",
       "      <td>angry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>не надо не надо не надо не надо</td>\n",
       "      <td>angry</td>\n",
       "      <td>fa136da095807ea6cd18dd6e2f58d4d0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f9438ef68395c70a8714dc373a49d11</td>\n",
       "      <td>wavs/2f9438ef68395c70a8714dc373a49d11.wav</td>\n",
       "      <td>4.640000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>фозил кори mp три</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3d436884cbbe25373914f8768de494f7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9937036a9c0dba20eecbffddd00f2be2</td>\n",
       "      <td>wavs/9937036a9c0dba20eecbffddd00f2be2.wav</td>\n",
       "      <td>4.341750</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.0</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fb0ae78586a235018103acec22a80a8f</td>\n",
       "      <td>wavs/fb0ae78586a235018103acec22a80a8f.wav</td>\n",
       "      <td>3.900562</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>сколько стоит на керамбит</td>\n",
       "      <td>neutral</td>\n",
       "      <td>80bc833cf6b3f106d2e8991783a31e2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196dcf9e1aaac46c2aee45e7f6adfb92</td>\n",
       "      <td>wavs/196dcf9e1aaac46c2aee45e7f6adfb92.wav</td>\n",
       "      <td>4.780000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>афина когда закончится эта телепередача</td>\n",
       "      <td>neutral</td>\n",
       "      <td>bd78f079676fa5f1ed17253c9a440cc6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            hash_id  \\\n",
       "0  475e76f77ac1ed7cabafca740b15b32a   \n",
       "1  2f9438ef68395c70a8714dc373a49d11   \n",
       "2  9937036a9c0dba20eecbffddd00f2be2   \n",
       "3  fb0ae78586a235018103acec22a80a8f   \n",
       "4  196dcf9e1aaac46c2aee45e7f6adfb92   \n",
       "\n",
       "                                  audio_path  duration annotator_emo  \\\n",
       "0  wavs/475e76f77ac1ed7cabafca740b15b32a.wav  2.453000         angry   \n",
       "1  wavs/2f9438ef68395c70a8714dc373a49d11.wav  4.640000       neutral   \n",
       "2  wavs/9937036a9c0dba20eecbffddd00f2be2.wav  4.341750       neutral   \n",
       "3  wavs/fb0ae78586a235018103acec22a80a8f.wav  3.900562       neutral   \n",
       "4  wavs/196dcf9e1aaac46c2aee45e7f6adfb92.wav  4.780000       neutral   \n",
       "\n",
       "   golden_emo                      annotator_id  \\\n",
       "0         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "1         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "2         2.0  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "3         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "4         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "\n",
       "                              speaker_text speaker_emo  \\\n",
       "0          не надо не надо не надо не надо       angry   \n",
       "1                        фозил кори mp три     neutral   \n",
       "2                                      NaN         NaN   \n",
       "3                сколько стоит на керамбит     neutral   \n",
       "4  афина когда закончится эта телепередача     neutral   \n",
       "\n",
       "                          source_id  \n",
       "0  fa136da095807ea6cd18dd6e2f58d4d0  \n",
       "1  3d436884cbbe25373914f8768de494f7  \n",
       "2                               NaN  \n",
       "3  80bc833cf6b3f106d2e8991783a31e2b  \n",
       "4  bd78f079676fa5f1ed17253c9a440cc6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79088, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_id</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>duration</th>\n",
       "      <th>annotator_emo</th>\n",
       "      <th>golden_emo</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>speaker_emo</th>\n",
       "      <th>source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e9961c53ca6eeb440b217e539fbf46c</td>\n",
       "      <td>wavs/9e9961c53ca6eeb440b217e539fbf46c.wav</td>\n",
       "      <td>5.82</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>я слушаю</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4282ddc30d71ef420e202e0c60391e9f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0166f65a30354db8282682b1a280e64c</td>\n",
       "      <td>wavs/0166f65a30354db8282682b1a280e64c.wav</td>\n",
       "      <td>3.70</td>\n",
       "      <td>sad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>каким стал сбер</td>\n",
       "      <td>neutral</td>\n",
       "      <td>d70dc98ed56e9362eaefefb7b2827c8f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d49a6b560155831725a7bdc7d0a96099</td>\n",
       "      <td>wavs/d49a6b560155831725a7bdc7d0a96099.wav</td>\n",
       "      <td>4.38</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>где родился шерлок холмс</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0ee35d2abecf4272ecc8e1539b0839d8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c6852b0925797612d7b6724da8cbe7b4</td>\n",
       "      <td>wavs/c6852b0925797612d7b6724da8cbe7b4.wav</td>\n",
       "      <td>8.58</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>открой в браузере ennio morricone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0855e363c1787df1592f58f7a27ebe13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0166f65a30354db8282682b1a280e64c</td>\n",
       "      <td>wavs/0166f65a30354db8282682b1a280e64c.wav</td>\n",
       "      <td>3.70</td>\n",
       "      <td>sad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a5562e26cd8f1949488a2d1e1e549d97</td>\n",
       "      <td>каким стал сбер</td>\n",
       "      <td>neutral</td>\n",
       "      <td>d70dc98ed56e9362eaefefb7b2827c8f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            hash_id  \\\n",
       "0  9e9961c53ca6eeb440b217e539fbf46c   \n",
       "1  0166f65a30354db8282682b1a280e64c   \n",
       "2  d49a6b560155831725a7bdc7d0a96099   \n",
       "3  c6852b0925797612d7b6724da8cbe7b4   \n",
       "4  0166f65a30354db8282682b1a280e64c   \n",
       "\n",
       "                                  audio_path  duration annotator_emo  \\\n",
       "0  wavs/9e9961c53ca6eeb440b217e539fbf46c.wav      5.82       neutral   \n",
       "1  wavs/0166f65a30354db8282682b1a280e64c.wav      3.70           sad   \n",
       "2  wavs/d49a6b560155831725a7bdc7d0a96099.wav      4.38       neutral   \n",
       "3  wavs/c6852b0925797612d7b6724da8cbe7b4.wav      8.58       neutral   \n",
       "4  wavs/0166f65a30354db8282682b1a280e64c.wav      3.70           sad   \n",
       "\n",
       "   golden_emo                      annotator_id  \\\n",
       "0         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "1         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "2         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "3         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "4         NaN  a5562e26cd8f1949488a2d1e1e549d97   \n",
       "\n",
       "                        speaker_text speaker_emo  \\\n",
       "0                           я слушаю     neutral   \n",
       "1                    каким стал сбер     neutral   \n",
       "2           где родился шерлок холмс     neutral   \n",
       "3  открой в браузере ennio morricone     neutral   \n",
       "4                    каким стал сбер     neutral   \n",
       "\n",
       "                          source_id  \n",
       "0  4282ddc30d71ef420e202e0c60391e9f  \n",
       "1  d70dc98ed56e9362eaefefb7b2827c8f  \n",
       "2  0ee35d2abecf4272ecc8e1539b0839d8  \n",
       "3  0855e363c1787df1592f58f7a27ebe13  \n",
       "4  d70dc98ed56e9362eaefefb7b2827c8f  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_crowd_train = pd.read_csv(DUSHA_CROWD_TRAIN_FILE_PATH, sep=TAB)\n",
    "print(raw_crowd_train.shape)\n",
    "display(raw_crowd_train.head())\n",
    "\n",
    "raw_crowd_test = pd.read_csv(DUSHA_CROWD_TEST_FILE_PATH, sep=TAB)\n",
    "print(raw_crowd_test.shape)\n",
    "display(raw_crowd_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_id</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>duration</th>\n",
       "      <th>annotator_emo</th>\n",
       "      <th>golden_emo</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>speaker_emo</th>\n",
       "      <th>source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>412403</th>\n",
       "      <td>5d9560dd2cba88b2dc87b6b4d5b6a29d</td>\n",
       "      <td>wavs/5d9560dd2cba88b2dc87b6b4d5b6a29d.wav</td>\n",
       "      <td>0.347875</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09184134bd1ddeb646205ba8e981fba8</td>\n",
       "      <td>фильмы меньшова</td>\n",
       "      <td>sad</td>\n",
       "      <td>24725b876b5e72993ec6c35688f754b8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412437</th>\n",
       "      <td>5d9560dd2cba88b2dc87b6b4d5b6a29d</td>\n",
       "      <td>wavs/5d9560dd2cba88b2dc87b6b4d5b6a29d.wav</td>\n",
       "      <td>0.347875</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>076ffc89109d8d0cb8727de8f75b5c94</td>\n",
       "      <td>фильмы меньшова</td>\n",
       "      <td>sad</td>\n",
       "      <td>24725b876b5e72993ec6c35688f754b8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412457</th>\n",
       "      <td>5d9560dd2cba88b2dc87b6b4d5b6a29d</td>\n",
       "      <td>wavs/5d9560dd2cba88b2dc87b6b4d5b6a29d.wav</td>\n",
       "      <td>0.347875</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62942acb4975e3cac00d06726a0dfd83</td>\n",
       "      <td>фильмы меньшова</td>\n",
       "      <td>sad</td>\n",
       "      <td>24725b876b5e72993ec6c35688f754b8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414061</th>\n",
       "      <td>5d9560dd2cba88b2dc87b6b4d5b6a29d</td>\n",
       "      <td>wavs/5d9560dd2cba88b2dc87b6b4d5b6a29d.wav</td>\n",
       "      <td>0.347875</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a30fefe82e1e460f186efe6e9bbf9c58</td>\n",
       "      <td>фильмы меньшова</td>\n",
       "      <td>sad</td>\n",
       "      <td>24725b876b5e72993ec6c35688f754b8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 hash_id  \\\n",
       "412403  5d9560dd2cba88b2dc87b6b4d5b6a29d   \n",
       "412437  5d9560dd2cba88b2dc87b6b4d5b6a29d   \n",
       "412457  5d9560dd2cba88b2dc87b6b4d5b6a29d   \n",
       "414061  5d9560dd2cba88b2dc87b6b4d5b6a29d   \n",
       "\n",
       "                                       audio_path  duration annotator_emo  \\\n",
       "412403  wavs/5d9560dd2cba88b2dc87b6b4d5b6a29d.wav  0.347875         other   \n",
       "412437  wavs/5d9560dd2cba88b2dc87b6b4d5b6a29d.wav  0.347875         other   \n",
       "412457  wavs/5d9560dd2cba88b2dc87b6b4d5b6a29d.wav  0.347875         other   \n",
       "414061  wavs/5d9560dd2cba88b2dc87b6b4d5b6a29d.wav  0.347875         other   \n",
       "\n",
       "        golden_emo                      annotator_id     speaker_text  \\\n",
       "412403         NaN  09184134bd1ddeb646205ba8e981fba8  фильмы меньшова   \n",
       "412437         NaN  076ffc89109d8d0cb8727de8f75b5c94  фильмы меньшова   \n",
       "412457         NaN  62942acb4975e3cac00d06726a0dfd83  фильмы меньшова   \n",
       "414061         NaN  a30fefe82e1e460f186efe6e9bbf9c58  фильмы меньшова   \n",
       "\n",
       "       speaker_emo                         source_id  \n",
       "412403         sad  24725b876b5e72993ec6c35688f754b8  \n",
       "412437         sad  24725b876b5e72993ec6c35688f754b8  \n",
       "412457         sad  24725b876b5e72993ec6c35688f754b8  \n",
       "414061         sad  24725b876b5e72993ec6c35688f754b8  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_crowd_train[raw_crowd_train.hash_id == '5d9560dd2cba88b2dc87b6b4d5b6a29d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17217"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_crowd_test.hash_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>annotator_emo</th>\n",
       "      <th>annotator_emo_aggregated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hash_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>6</th>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sad</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          annotator_emo annotator_emo_aggregated\n",
       "hash_id                                         \n",
       "3       6           sad                      sad\n",
       "        7           sad                      sad"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'hash_id': [1, 1, 1, 2, 2, 2, 3, 3],\n",
    "    'annotator_emo': ['happy', 'happy', 'sad', 'neutral', 'neutral', 'happy', 'sad', 'sad']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# Apply the filtering function group-wise and drop rows where condition isn't met\n",
    "aggregate_by_mode_annotator:Callable = partial(aggregate_by_mode, by_col_name=ANNOTATOR_EMOION_FIELD_NAME, aggregated_keyword=ANNOTATOR_AGGREGATED_FIELD_NAME)\n",
    "filtered_df = df.groupby(by=HASH_ID_COLUMN_NAME).apply(aggregate_by_mode_annotator, include_groups=False).dropna(subset=[ANNOTATOR_AGGREGATED_FIELD_NAME])\n",
    "\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14946/3907367092.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df:pd.DataFrame = raw_crowd_test.groupby(HASH_ID_COLUMN_NAME).apply(filter_by_mode_annotator).dropna(subset=[AGGREGATED_KEYWORD])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(62463, 10)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df:pd.DataFrame = raw_crowd_test.groupby(HASH_ID_COLUMN_NAME).apply(aggregate_by_mode_annotator).dropna(subset=[AGGREGATED_KEYWORD])\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "aggregate_by_mode() missing 1 required positional argument: 'agg_col_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/groupby.py:1824\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1824\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1825\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1826\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj, Series)\n\u001b[32m   1827\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1828\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.shape != \u001b[38;5;28mself\u001b[39m._obj_with_exclusions.shape\n\u001b[32m   1829\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/groupby.py:1885\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1859\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1860\u001b[39m \u001b[33;03mApply function f in python space\u001b[39;00m\n\u001b[32m   1861\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1883\u001b[39m \u001b[33;03m    data after applying f\u001b[39;00m\n\u001b[32m   1884\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/ops.py:919\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    918\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n",
      "\u001b[31mTypeError\u001b[39m: aggregate_by_mode() missing 1 required positional argument: 'agg_col_name'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[138]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m raw_crowd_test_agged:pd.DataFrame = \u001b[43maggregate_crowd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_crowd_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m raw_crowd_test_agged.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:4\u001b[39m, in \u001b[36maggregate_crowd\u001b[39m\u001b[34m(df, agg_func, agg_col_name, by_col_name, aggregated_col_name)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/generic.py:230\u001b[39m, in \u001b[36mSeriesGroupBy.apply\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(\n\u001b[32m    225\u001b[39m     _apply_docs[\u001b[33m\"\u001b[39m\u001b[33mtemplate\u001b[39m\u001b[33m\"\u001b[39m].format(\n\u001b[32m    226\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mseries\u001b[39m\u001b[33m\"\u001b[39m, examples=_apply_docs[\u001b[33m\"\u001b[39m\u001b[33mseries_examples\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    227\u001b[39m     )\n\u001b[32m    228\u001b[39m )\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, *args, **kwargs) -> Series:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/groupby.py:1846\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1830\u001b[39m             warnings.warn(\n\u001b[32m   1831\u001b[39m                 message=_apply_groupings_depr.format(\n\u001b[32m   1832\u001b[39m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1835\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   1836\u001b[39m             )\n\u001b[32m   1837\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1838\u001b[39m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[32m   1839\u001b[39m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1843\u001b[39m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[32m   1844\u001b[39m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1846\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_obj_with_exclusions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/groupby.py:1885\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1850\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1851\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1852\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1857\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1858\u001b[39m ) -> NDFrameT:\n\u001b[32m   1859\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1860\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1861\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1883\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1884\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1887\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/ops.py:919\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    918\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    921\u001b[39m     mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: aggregate_by_mode() missing 1 required positional argument: 'agg_col_name'"
     ]
    }
   ],
   "source": [
    "raw_crowd_test_agged:pd.DataFrame = aggregate_crowd(df=raw_crowd_test)\n",
    "raw_crowd_test_agged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13208"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_crowd_test_agged.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184633, 17217)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_crowd_train.hash_id.unique()), len(raw_crowd_test.hash_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_HLF_file(\n",
    "    HLF_file_path:Path = PROCESSED_DUSHA_CROWD_TRAIN_HLF_STABLE_VERSION_FILE_PATH,\n",
    "    )->List[HashHLF]:\n",
    "    hash_HLF_list:List[HashHLF] = []\n",
    "    with open(HLF_file_path) as f:\n",
    "        for line in f:\n",
    "            el:Optional[HashHLF] = eval(eval(line)) if eval(line) is not None else None\n",
    "            if el is not None:\n",
    "                hash_HLF_list.append(el)\n",
    "                \n",
    "    return hash_HLF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_crowd_2_raw_crowd_HLF_table_format(\n",
    "    raw_crowd:pd.DataFrame,\n",
    "    agg_col_name:str = ANNOTATOR_AGGREGATED_FIELD_NAME,\n",
    "    ):\n",
    "    raw_crowd_test_agged:pd.DataFrame = aggregate_crowd(\n",
    "        df=raw_crowd, \n",
    "        aggregated_col_name=agg_col_name,\n",
    "    )\n",
    "\n",
    "    raw_crowd_unique_hashes:pd.DataFrame = raw_crowd_test_agged[~raw_crowd_test_agged.index.duplicated()]\n",
    "    # raw_crowd_train_unique_hashes_only_goldens:pd.DataFrame = raw_crowd_train_unique_hashes[~raw_crowd_train_unique_hashes.golden_emo.isna()]\n",
    "    # raw_crowd_unique_hashes_with_speaker_emo:pd.DataFrame = raw_crowd_unique_hashes[~raw_crowd_unique_hashes.speaker_emo.isna()]\n",
    "    # raw_crowd_unique_hashes_with_speaker_emo_with_speaker_text:pd.DataFrame = raw_crowd_unique_hashes_with_speaker_emo[~raw_crowd_unique_hashes_with_speaker_emo[agg_col_name].isna()]\n",
    "    # raw_crowd_unique_hashes_with_speaker_emo_with_speaker_text.set_index(HASH_ID_COLUMN_NAME, drop=True, inplace=True)\n",
    "    # raw_crowd_train_unique_hashes_only_goldens_with_speaker_text.golden_emo = raw_crowd_train_unique_hashes_only_goldens_with_speaker_text.golden_emo.apply(lambda x: GoldenEmo(round(x)).name)\n",
    "    # raw_crowd_train_unique_hashes_only_goldens.index.name=None\n",
    "    return raw_crowd_unique_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  aggregated    1\n",
      "B  aggregated    2\n",
      "C  aggregated    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define the series\n",
    "series = pd.Series(\n",
    "    index=['A', 'A', 'B', 'B', 'B', 'C', 'C'],\n",
    "    data=[1, 1, 2, 2, 3, 3, 3],\n",
    ")\n",
    "# Group by index and apply the function\n",
    "grouped = series.groupby(series.index).apply(aggregate_by_mode)\n",
    "\n",
    "# Drop None values and keep only aggregated values\n",
    "# result = grouped.dropna()\n",
    "\n",
    "print(grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0004eb46b532c20b2f181f1a65d3ff17    neutral\n",
       "0006985a7b739719a77be07ff64a4274    neutral\n",
       "00075643862a22ee402edd404c4fee93       None\n",
       "00117e5b11d1490c25eea137983ca994    neutral\n",
       "00173c4925e5a20cd1495742505ec0d2        sad\n",
       "dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_id_2_annotator_emo:pd.Series = pd.Series(index=raw_crowd_test.hash_id.to_list(), data=raw_crowd_test.annotator_emo.to_list()).sort_index()\n",
    "grouped = hash_id_2_annotator_emo.groupby(hash_id_2_annotator_emo.index).apply(aggregate_by_mode)\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HLF_withspeaker_emottions_table(\n",
    "    raw_crowd:pd.DataFrame,\n",
    "    HLF_file_path:Path,\n",
    "    agg_col_name:str = ANNOTATOR_AGGREGATED_FIELD_NAME,\n",
    "    )->pd.DataFrame:\n",
    "    hash_HLF_list:List[HashHLF] = read_HLF_file(HLF_file_path=HLF_file_path)\n",
    "    HLF_table:pd.DataFrame = hash_HLF_list_2_df(l=hash_HLF_list)\n",
    "    raw_crowd_unique_hashes_with_speaker_emo_with_speaker_text:pd.DataFrame = raw_crowd_2_raw_crowd_HLF_table_format(raw_crowd=raw_crowd, agg_col_name=agg_col_name)\n",
    "\n",
    "    HLF_with_speaker_emotions:pd.DataFrame = HLF_table.join(raw_crowd_unique_hashes_with_speaker_emo_with_speaker_text[agg_col_name], how=JOIN_HOW_INNER)\n",
    "    return HLF_with_speaker_emotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m HLF_with_speaker_emotions_train:pd.DataFrame = \u001b[43mHLF_withspeaker_emottions_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_crowd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_crowd_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mHLF_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPROCESSED_DUSHA_CROWD_TRAIN_HLF_LAST_VERSION_FILE_PATH\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(HLF_with_speaker_emotions_train.shape)\n\u001b[32m      6\u001b[39m display(HLF_with_speaker_emotions_train.head())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mHLF_withspeaker_emottions_table\u001b[39m\u001b[34m(raw_crowd, HLF_file_path, agg_col_name)\u001b[39m\n\u001b[32m      6\u001b[39m hash_HLF_list:List[HashHLF] = read_HLF_file(HLF_file_path=HLF_file_path)\n\u001b[32m      7\u001b[39m HLF_table:pd.DataFrame = hash_HLF_list_2_df(l=hash_HLF_list)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m raw_crowd_unique_hashes_with_speaker_emo_with_speaker_text:pd.DataFrame = \u001b[43mraw_crowd_2_raw_crowd_HLF_table_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_crowd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_crowd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magg_col_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43magg_col_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m HLF_with_speaker_emotions:pd.DataFrame = HLF_table.join(raw_crowd_unique_hashes_with_speaker_emo_with_speaker_text[agg_col_name], how=JOIN_HOW_INNER)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HLF_with_speaker_emotions\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[112]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mraw_crowd_2_raw_crowd_HLF_table_format\u001b[39m\u001b[34m(raw_crowd, agg_col_name)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraw_crowd_2_raw_crowd_HLF_table_format\u001b[39m(\n\u001b[32m      2\u001b[39m     raw_crowd:pd.DataFrame,\n\u001b[32m      3\u001b[39m     agg_col_name:\u001b[38;5;28mstr\u001b[39m = ANNOTATOR_AGGREGATED_FIELD_NAME,\n\u001b[32m      4\u001b[39m     ):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     raw_crowd_test_agged:pd.DataFrame = \u001b[43maggregate_crowd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_crowd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43maggregated_col_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43magg_col_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     raw_crowd_unique_hashes:pd.DataFrame = raw_crowd_test_agged[~raw_crowd_test_agged.index.duplicated()]\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# raw_crowd_train_unique_hashes_only_goldens:pd.DataFrame = raw_crowd_train_unique_hashes[~raw_crowd_train_unique_hashes.golden_emo.isna()]\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# raw_crowd_unique_hashes_with_speaker_emo:pd.DataFrame = raw_crowd_unique_hashes[~raw_crowd_unique_hashes.speaker_emo.isna()]\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# raw_crowd_unique_hashes_with_speaker_emo_with_speaker_text:pd.DataFrame = raw_crowd_unique_hashes_with_speaker_emo[~raw_crowd_unique_hashes_with_speaker_emo[agg_col_name].isna()]\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# raw_crowd_unique_hashes_with_speaker_emo_with_speaker_text.set_index(HASH_ID_COLUMN_NAME, drop=True, inplace=True)\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# raw_crowd_train_unique_hashes_only_goldens_with_speaker_text.golden_emo = raw_crowd_train_unique_hashes_only_goldens_with_speaker_text.golden_emo.apply(lambda x: GoldenEmo(round(x)).name)\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# raw_crowd_train_unique_hashes_only_goldens.index.name=None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:3\u001b[39m, in \u001b[36maggregate_crowd\u001b[39m\u001b[34m(df, agg_func, agg_col_name, by_col_name, aggregated_col_name)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/groupby.py:1819\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1816\u001b[39m     f = func\n\u001b[32m   1818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_groups:\n\u001b[32m-> \u001b[39m\u001b[32m1819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_obj_with_exclusions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m   1822\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/groupby.py:1889\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1886\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1887\u001b[39m     not_indexed_same = mutated\n\u001b[32m-> \u001b[39m\u001b[32m1889\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrap_applied_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnot_indexed_same\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1894\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/generic.py:1584\u001b[39m, in \u001b[36mDataFrameGroupBy._wrap_applied_output\u001b[39m\u001b[34m(self, data, values, not_indexed_same, is_transform)\u001b[39m\n\u001b[32m   1582\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._constructor()\n\u001b[32m   1583\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_not_none, DataFrame):\n\u001b[32m-> \u001b[39m\u001b[32m1584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concat_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnot_indexed_same\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnot_indexed_same\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1590\u001b[39m key_index = \u001b[38;5;28mself\u001b[39m._grouper.result_index \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.as_index \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_not_none, (np.ndarray, Index)):\n\u001b[32m   1593\u001b[39m     \u001b[38;5;66;03m# GH#1738: values is list of arrays of unequal lengths\u001b[39;00m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;66;03m#  fall through to the outer else clause\u001b[39;00m\n\u001b[32m   1595\u001b[39m     \u001b[38;5;66;03m# TODO: sure this is right?  we used to do this\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;66;03m#  after raising AttributeError above\u001b[39;00m\n\u001b[32m   1597\u001b[39m     \u001b[38;5;66;03m# GH 18930\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/groupby/groupby.py:1457\u001b[39m, in \u001b[36mGroupBy._concat_objects\u001b[39m\u001b[34m(self, values, not_indexed_same, is_transform)\u001b[39m\n\u001b[32m   1454\u001b[39m     group_levels = \u001b[38;5;28mself\u001b[39m._grouper.levels\n\u001b[32m   1455\u001b[39m     group_names = \u001b[38;5;28mself\u001b[39m._grouper.names\n\u001b[32m-> \u001b[39m\u001b[32m1457\u001b[39m     result = \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1466\u001b[39m     \u001b[38;5;66;03m# GH5610, returns a MI, with the first level being a\u001b[39;00m\n\u001b[32m   1467\u001b[39m     \u001b[38;5;66;03m# range index\u001b[39;00m\n\u001b[32m   1468\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(values)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/reshape/concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/reshape/concat.py:671\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.objs:\n\u001b[32m    670\u001b[39m     indexers = {}\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ax, new_labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m):\n\u001b[32m    672\u001b[39m         \u001b[38;5;66;03m# ::-1 to convert BlockManager ax to DataFrame ax\u001b[39;00m\n\u001b[32m    673\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ax == \u001b[38;5;28mself\u001b[39m.bm_axis:\n\u001b[32m    674\u001b[39m             \u001b[38;5;66;03m# Suppress reindexing on concat axis\u001b[39;00m\n\u001b[32m    675\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mproperties.pyx:36\u001b[39m, in \u001b[36mpandas._libs.properties.CachedProperty.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/reshape/concat.py:702\u001b[39m, in \u001b[36m_Concatenator.new_axes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    699\u001b[39m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_axes\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Index]:\n\u001b[32m    701\u001b[39m     ndim = \u001b[38;5;28mself\u001b[39m._get_result_dim()\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_concat_axis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_comb_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/reshape/concat.py:703\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    699\u001b[39m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_axes\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Index]:\n\u001b[32m    701\u001b[39m     ndim = \u001b[38;5;28mself\u001b[39m._get_result_dim()\n\u001b[32m    702\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_concat_axis\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[38;5;28mself\u001b[39m.bm_axis \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_comb_axis(i)\n\u001b[32m    704\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ndim)\n\u001b[32m    705\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mproperties.pyx:36\u001b[39m, in \u001b[36mpandas._libs.properties.CachedProperty.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/reshape/concat.py:762\u001b[39m, in \u001b[36m_Concatenator._get_concat_axis\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    760\u001b[39m     concat_axis = _concat_indexes(indexes)\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m     concat_axis = \u001b[43m_make_concat_multiindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnames\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[38;5;28mself\u001b[39m._maybe_check_integrity(concat_axis)\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concat_axis\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/reshape/concat.py:821\u001b[39m, in \u001b[36m_make_concat_multiindex\u001b[39m\u001b[34m(indexes, keys, levels, names)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(hlevel, indexes):\n\u001b[32m    820\u001b[39m         \u001b[38;5;66;03m# Find matching codes, include matching nan values as equal.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m         mask = (isna(level) & isna(key)) | (\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m)\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mask.any():\n\u001b[32m    823\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/arraylike.py:40\u001b[39m, in \u001b[36mOpsMixin.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__eq__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/indexes/base.py:7201\u001b[39m, in \u001b[36mIndex._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   7197\u001b[39m     result = op(\u001b[38;5;28mself\u001b[39m._values, other)\n\u001b[32m   7199\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(\u001b[38;5;28mself\u001b[39m.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCMultiIndex):\n\u001b[32m   7200\u001b[39m     \u001b[38;5;66;03m# don't pass MultiIndex\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m7201\u001b[39m     result = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7203\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   7204\u001b[39m     result = ops.comparison_op(\u001b[38;5;28mself\u001b[39m._values, other, op)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/pandas/core/ops/array_ops.py:129\u001b[39m, in \u001b[36mcomp_method_OBJECT_ARRAY\u001b[39m\u001b[34m(op, x, y)\u001b[39m\n\u001b[32m    127\u001b[39m     result = libops.vec_compare(x.ravel(), y.ravel(), op)\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     result = \u001b[43mlibops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape(x.shape)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "HLF_with_speaker_emotions_train:pd.DataFrame = HLF_withspeaker_emottions_table(\n",
    "    raw_crowd=raw_crowd_train,\n",
    "    HLF_file_path=PROCESSED_DUSHA_CROWD_TRAIN_HLF_STABLE_VERSION_FILE_PATH\n",
    ")\n",
    "print(HLF_with_speaker_emotions_train.shape)\n",
    "display(HLF_with_speaker_emotions_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HLF_with_speaker_emotions_train.to_csv(PROCESSED_DUSHA_SER_TABLES_TRAIN_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13208, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loudness</th>\n",
       "      <th>HF_power_ratio</th>\n",
       "      <th>pronounce_speed_WPS</th>\n",
       "      <th>pronounce_speed_LPS</th>\n",
       "      <th>pronounce_speed_SPS</th>\n",
       "      <th>transcription_features_mean_words_length</th>\n",
       "      <th>transcription_features_profanity_words_ratio</th>\n",
       "      <th>transcription_features_meaning</th>\n",
       "      <th>transcription_features_POS_ratio</th>\n",
       "      <th>annotator_emo_aggregated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9e9961c53ca6eeb440b217e539fbf46c</th>\n",
       "      <td>51.384979</td>\n",
       "      <td>0.132317</td>\n",
       "      <td>1.202749</td>\n",
       "      <td>1.202749</td>\n",
       "      <td>0.687285</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0166f65a30354db8282682b1a280e64c</th>\n",
       "      <td>39.728794</td>\n",
       "      <td>0.249508</td>\n",
       "      <td>3.513514</td>\n",
       "      <td>3.513514</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d49a6b560155831725a7bdc7d0a96099</th>\n",
       "      <td>52.689034</td>\n",
       "      <td>0.029966</td>\n",
       "      <td>4.794521</td>\n",
       "      <td>4.794521</td>\n",
       "      <td>1.598174</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c6852b0925797612d7b6724da8cbe7b4</th>\n",
       "      <td>63.896108</td>\n",
       "      <td>0.014975</td>\n",
       "      <td>3.379953</td>\n",
       "      <td>3.379953</td>\n",
       "      <td>0.699301</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33152583dcbf1fe40d142954a2a7ca23</th>\n",
       "      <td>68.913476</td>\n",
       "      <td>0.004888</td>\n",
       "      <td>5.657895</td>\n",
       "      <td>5.657895</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   loudness  HF_power_ratio  \\\n",
       "9e9961c53ca6eeb440b217e539fbf46c  51.384979        0.132317   \n",
       "0166f65a30354db8282682b1a280e64c  39.728794        0.249508   \n",
       "d49a6b560155831725a7bdc7d0a96099  52.689034        0.029966   \n",
       "c6852b0925797612d7b6724da8cbe7b4  63.896108        0.014975   \n",
       "33152583dcbf1fe40d142954a2a7ca23  68.913476        0.004888   \n",
       "\n",
       "                                  pronounce_speed_WPS  pronounce_speed_LPS  \\\n",
       "9e9961c53ca6eeb440b217e539fbf46c             1.202749             1.202749   \n",
       "0166f65a30354db8282682b1a280e64c             3.513514             3.513514   \n",
       "d49a6b560155831725a7bdc7d0a96099             4.794521             4.794521   \n",
       "c6852b0925797612d7b6724da8cbe7b4             3.379953             3.379953   \n",
       "33152583dcbf1fe40d142954a2a7ca23             5.657895             5.657895   \n",
       "\n",
       "                                  pronounce_speed_SPS  \\\n",
       "9e9961c53ca6eeb440b217e539fbf46c             0.687285   \n",
       "0166f65a30354db8282682b1a280e64c             1.081081   \n",
       "d49a6b560155831725a7bdc7d0a96099             1.598174   \n",
       "c6852b0925797612d7b6724da8cbe7b4             0.699301   \n",
       "33152583dcbf1fe40d142954a2a7ca23             2.500000   \n",
       "\n",
       "                                  transcription_features_mean_words_length  \\\n",
       "9e9961c53ca6eeb440b217e539fbf46c                                  3.500000   \n",
       "0166f65a30354db8282682b1a280e64c                                  4.333333   \n",
       "d49a6b560155831725a7bdc7d0a96099                                  5.250000   \n",
       "c6852b0925797612d7b6724da8cbe7b4                                  5.800000   \n",
       "33152583dcbf1fe40d142954a2a7ca23                                  4.300000   \n",
       "\n",
       "                                  transcription_features_profanity_words_ratio  \\\n",
       "9e9961c53ca6eeb440b217e539fbf46c                                           0.0   \n",
       "0166f65a30354db8282682b1a280e64c                                           0.0   \n",
       "d49a6b560155831725a7bdc7d0a96099                                           0.0   \n",
       "c6852b0925797612d7b6724da8cbe7b4                                           0.0   \n",
       "33152583dcbf1fe40d142954a2a7ca23                                           0.0   \n",
       "\n",
       "                                 transcription_features_meaning  \\\n",
       "9e9961c53ca6eeb440b217e539fbf46c                           None   \n",
       "0166f65a30354db8282682b1a280e64c                           None   \n",
       "d49a6b560155831725a7bdc7d0a96099                           None   \n",
       "c6852b0925797612d7b6724da8cbe7b4                           None   \n",
       "33152583dcbf1fe40d142954a2a7ca23                           None   \n",
       "\n",
       "                                 transcription_features_POS_ratio  \\\n",
       "9e9961c53ca6eeb440b217e539fbf46c                             None   \n",
       "0166f65a30354db8282682b1a280e64c                             None   \n",
       "d49a6b560155831725a7bdc7d0a96099                             None   \n",
       "c6852b0925797612d7b6724da8cbe7b4                             None   \n",
       "33152583dcbf1fe40d142954a2a7ca23                             None   \n",
       "\n",
       "                                 annotator_emo_aggregated  \n",
       "9e9961c53ca6eeb440b217e539fbf46c                  neutral  \n",
       "0166f65a30354db8282682b1a280e64c                      sad  \n",
       "d49a6b560155831725a7bdc7d0a96099                  neutral  \n",
       "c6852b0925797612d7b6724da8cbe7b4                  neutral  \n",
       "33152583dcbf1fe40d142954a2a7ca23                  neutral  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HLF_with_speaker_emotions_test:pd.DataFrame = HLF_withspeaker_emottions_table(\n",
    "    raw_crowd=raw_crowd_test,\n",
    "    HLF_file_path=PROCESSED_DUSHA_CROWD_TEST_HLF_LAST_VERSION_FILE_PATH\n",
    ")\n",
    "print(HLF_with_speaker_emotions_test.shape)\n",
    "HLF_with_speaker_emotions_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HLF_with_speaker_emotions_test.to_csv(PROCESSED_DUSHA_SER_TABLES_TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.55      0.40      0.46      2853\n",
      "     neutral       0.48      0.89      0.62      7462\n",
      "    positive       0.33      0.00      0.00      2279\n",
      "         sad       0.37      0.10      0.16      4623\n",
      "\n",
      "    accuracy                           0.48     17217\n",
      "   macro avg       0.43      0.35      0.31     17217\n",
      "weighted avg       0.44      0.48      0.39     17217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = HLF_with_speaker_emotions_train.drop(columns=[ANNOTATOR_AGGREGATED_FIELD_NAME])  \n",
    "y_train = HLF_with_speaker_emotions_train[SPEAKER_EMOTION_FIELD_NAME]                \n",
    "\n",
    "X_test = HLF_with_speaker_emotions_test.drop(columns=[ANNOTATOR_AGGREGATED_FIELD_NAME])   \n",
    "y_test = HLF_with_speaker_emotions_test[SPEAKER_EMOTION_FIELD_NAME]                  \n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "model = CatBoostClassifier(\n",
    "    iterations=100,       # Number of boosting iterations\n",
    "    learning_rate=0.1,    # Learning rate\n",
    "    depth=6,              # Depth of the trees\n",
    "    verbose=0           # Print progress every 100 iterations\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# If you have the true labels for the test set, evaluate accuracy\n",
    "if SPEAKER_EMOTION_FIELD_NAME in HLF_with_speaker_emotions_test.columns:\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transcription_features_profanity_words_ratio    0.248369\n",
       "transcription_features_mean_words_length        0.241024\n",
       "loudness                                        0.201275\n",
       "HF_power_ratio                                  0.108504\n",
       "pronounce_speed_SPS                             0.100302\n",
       "pronounce_speed_LPS                             0.051853\n",
       "pronounce_speed_WPS                             0.048673\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_feature_importance:pd.Series = pd.Series(index=X_train.columns, data=model.feature_importances_ / (sum(model.feature_importances_))).sort_values(ascending=False)\n",
    "normalized_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "# # Initialize model and tokenizer\n",
    "# ROSBERTA_EMBEDDER_MODEL_NAME:str = 'ai-forever/ru-en-RoSBERTa'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(ROSBERTA_EMBEDDER_MODEL_NAME)\n",
    "# model = AutoModel.from_pretrained(ROSBERTA_EMBEDDER_MODEL_NAME)\n",
    "\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output.last_hidden_state\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# # Russian text processing\n",
    "# texts = [\"Ваш текст на русском языке здесь\"]\n",
    "# encoded_input = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "\n",
    "# # Choose pooling method\n",
    "# embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "# # embeddings = model_output.last_hidden_state[:,0]  # CLS pooling alternative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(\n",
    "    model_output:transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions, \n",
    "    attention_mask:torch.Tensor,\n",
    "    )->torch.Tensor:\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=CLAMP_MIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at /data01/vvkiselev/data/other/dpl/models/ru-en-RoSBERTa and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path:Path = Path('/data01/vvkiselev/data/other/dpl/models/ru-en-RoSBERTa')\n",
    "tokenizer:transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast = AutoTokenizer.from_pretrained(model_path)\n",
    "model:transformers.models.roberta.modeling_roberta.RobertaModel = AutoModel.from_pretrained(model_path).to(ROSBERTA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_2_embeddings(\n",
    "    texts:List[str],\n",
    "    tokenizer:transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
    "    model:transformers.models.roberta.modeling_roberta.RobertaModel,\n",
    "    padding:bool=True,\n",
    "    truncation:bool=False,\n",
    "    return_tensors=TORCH_TENSORS_KEYWOED,\n",
    "    device=ROSBERTA_DEVICE,\n",
    "    attention_mask_keyword:str=ATTENTION_MASK_KEYWORD,\n",
    "    normalize_p:int = ROSBERTA_NORMALIZE_P,\n",
    "    normalize_dim:int = ROSBERTA_NORMALIZE_DIM,\n",
    "    )->torch.Tensor:\n",
    "    inputs:transformers.tokenization_utils_base.BatchEncoding = tokenizer(\n",
    "        texts,\n",
    "        padding=padding,\n",
    "        truncation=truncation,\n",
    "        return_tensors=return_tensors,\n",
    "    ).to(device)  # Move inputs to GPU\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs:transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions = model(**inputs)\n",
    "\n",
    "    # Apply manual pooling\n",
    "    sentence_embeddings:torch.Tensor = mean_pooling(\n",
    "        model_output=outputs, \n",
    "        attention_mask=inputs[attention_mask_keyword],\n",
    "    )\n",
    "    embeddings:torch.Tensor = torch.nn.functional.normalize(\n",
    "        sentence_embeddings, \n",
    "        p=normalize_p, \n",
    "        dim=normalize_dim,\n",
    "    )\n",
    "\n",
    "    # print(f\"Embedding shape: {embeddings.shape}\")  # Output: torch.Size([1, 1024])\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2112, 1024])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs:torch.Tensor = texts_2_embeddings(\n",
    "    # texts=['Пример русского текста для анализа','я пошел гулять'],\n",
    "    texts=list(filter(lambda x: isinstance(x, str), list(raw_crowd_train[SPEAKER_TEXT_FIELD_NAME].head(5000).unique()))),\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['не надо не надо не надо не надо',\n",
       " 'фозил кори mp три',\n",
       " nan,\n",
       " 'сколько стоит на керамбит',\n",
       " 'афина когда закончится эта телепередача']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_crowd_train.head()[SPEAKER_TEXT_FIELD_NAME].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcriptions_series_to_text_2_emb(\n",
    "    transcriptions_series:pd.Series,\n",
    "    tokenizer:transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
    "    model:transformers.models.roberta.modeling_roberta.RobertaModel,\n",
    "    num_chunks:int,\n",
    "    padding:bool=True,\n",
    "    truncation:bool=False,\n",
    "    return_tensors=TORCH_TENSORS_KEYWOED,\n",
    "    device=ROSBERTA_DEVICE,\n",
    "    attention_mask_keyword:str=ATTENTION_MASK_KEYWORD,\n",
    "    normalize_p:int = ROSBERTA_NORMALIZE_P,\n",
    "    normalize_dim:int = ROSBERTA_NORMALIZE_DIM,\n",
    "    )->Dict[str, torch.Tensor]:\n",
    "    unique_texts:List[str] = list(\n",
    "        filter(\n",
    "            lambda x: isinstance(x, str), \n",
    "            list(transcriptions_series.unique())\n",
    "        )\n",
    "    )\n",
    "    print(f'len(unique_texts) = {len(unique_texts)}')\n",
    "\n",
    "    chunks:List[List[str]] = divide_into_chunks(unique_texts, num_chunks)\n",
    "    unique_text_2_embedding:Dict[str, torch.Tensor] = {}\n",
    "    for chunk in tqdm(chunks):\n",
    "        chunk_embeddings:torch.Tensor = texts_2_embeddings(\n",
    "            texts=chunk,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            return_tensors=return_tensors,\n",
    "            device=device,\n",
    "            attention_mask_keyword=attention_mask_keyword,\n",
    "            normalize_p=normalize_p,\n",
    "            normalize_dim=normalize_dim,\n",
    "        ).cpu()\n",
    "        # print(f'chunk_embeddings.shape = {chunk_embeddings.shape}')\n",
    "        for chunk_i in range(len(chunk)):\n",
    "            unique_text_2_embedding[chunk[chunk_i]] = chunk_embeddings[chunk_i]\n",
    "\n",
    "    return unique_text_2_embedding\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(unique_texts) = 124568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:08<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "text_2_emb_train:Dict[str, torch.Tensor] = transcriptions_series_to_text_2_emb(\n",
    "    transcriptions_series=raw_crowd_train[SPEAKER_TEXT_FIELD_NAME],\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    num_chunks=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "не надо не надо не надо не надо                   tensor([ 0.0409,  0.0645, -0.0062,  ...,  0.01...\n",
       "фозил кори mp три                                 tensor([-0.0066,  0.0331,  0.0023,  ...,  0.01...\n",
       "сколько стоит на керамбит                         tensor([ 0.0246,  0.0035,  0.0029,  ..., -0.02...\n",
       "афина когда закончится эта телепередача           tensor([ 0.0242,  0.0469,  0.0188,  ...,  0.01...\n",
       "где проживают дети путина тихонова и воронцова    tensor([-0.0016, -0.0451, -0.0002,  ...,  0.05...\n",
       "dtype: object"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2_emb_series_train:pd.Series = pd.Series(index=text_2_emb_train.keys(), data=map(repr, text_2_emb_train.values()))\n",
    "text_2_emb_series_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_2_emb_train, PROCESSED_DUSHA_CROWD_TRAIN_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data01/vvkiselev/data/other/dpl/processed/dusha/crowd/train/text_embeddings/v1.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[332]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the dictionary from the file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m loaded_dict = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROCESSED_DUSHA_CROWD_TRAIN_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mlist\u001b[39m(loaded_dict.items())[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m].shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/data01/vvkiselev/data/other/dpl/processed/dusha/crowd/train/text_embeddings/v1.pt'"
     ]
    }
   ],
   "source": [
    "# Load the dictionary from the file\n",
    "# loaded_dict = torch.load(PROCESSED_DUSHA_CROWD_TRAIN_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH, weights_only=False)\n",
    "# list(loaded_dict.items())[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(unique_texts) = 16628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.82it/s]\n"
     ]
    }
   ],
   "source": [
    "text_2_emb_test:Dict[str, torch.Tensor] = transcriptions_series_to_text_2_emb(\n",
    "    transcriptions_series=raw_crowd_test[SPEAKER_TEXT_FIELD_NAME],\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    num_chunks=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_2_emb_test, PROCESSED_DUSHA_CROWD_TEST_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dict = torch.load(PROCESSED_DUSHA_CROWD_TEST_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH, weights_only=False)\n",
    "list(loaded_dict.items())[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# # Load the Russian language model\n",
    "# nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# # Sample Russian text\n",
    "# text = \"Я люблю программировать на Python.\"\n",
    "\n",
    "# # Process the text\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Iterate through tokens and print their text and POS\n",
    "# for token in doc:\n",
    "#     print(f\"Word: {token.text}, POS: {token.pos_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
