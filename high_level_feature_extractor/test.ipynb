{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import wave\n",
    "import sys\n",
    "import soundfile as sf\n",
    "# from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "from dataclasses import (\n",
    "    dataclass,\n",
    "    asdict,\n",
    ")\n",
    "from typing import (\n",
    "    Optional,\n",
    "    Callable,\n",
    "    Set,\n",
    "    Generator,\n",
    "    List,\n",
    "    Tuple,\n",
    "    Union,\n",
    "    Dict,\n",
    ")\n",
    "import time\n",
    "import dill\n",
    "import logging\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# from bdw.check import Check\n",
    "\n",
    "sys.path.append('..')\n",
    "from audio import (\n",
    "    Audio,\n",
    "    WAVFilePathInitArgs,\n",
    ")\n",
    "from text.profanity import (\n",
    "    PROFANITY_WORD_FILTER_LANG_NAME,\n",
    ")\n",
    "from configs.base import (\n",
    "    RB_FILE_READING_MODE,\n",
    "    SECONDS_QUANTITY_IN_MINUTE,\n",
    "    TAB,\n",
    "    RUSSIAN_VOWELS,\n",
    ")\n",
    "from configs.paths import (\n",
    "    DUSHA_CROWD_TRAIN_FILE_PATH,\n",
    "    DUSHA_CROWD_TEST_FILE_PATH,\n",
    "    DUSHA_CROWD_TRAIN_WAVS_DIR_PATH,\n",
    "    DUSHA_CROWD_TEST_WAVS_DIR_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TRAIN_HLF_LAST_VERSION_FILE_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TEST_HLF_LAST_VERSION_FILE_PATH,\n",
    ")\n",
    "from processing.text.normalization import (\n",
    "    normalized_tokens_2_normalized_text,\n",
    "    text_2_normalized_text,\n",
    ")\n",
    "from high_level_feature_extractor.text.profanity import (\n",
    "    text_2_is_contain_swear_words,\n",
    ")\n",
    "from high_level_feature_extractor.text.all import (\n",
    "    TranscriptionHighLevelFeatures,\n",
    ")\n",
    "from high_level_feature_extractor.extractor import (\n",
    "    PronounceSpeed,\n",
    ")\n",
    "from high_level_feature_extractor.extractor import (\n",
    "    HighLevelSpeechFeatures,\n",
    "    HashHLF,\n",
    ")\n",
    "from high_level_feature_extractor.extract import (\n",
    "    raw_crowd_2_HLF,\n",
    ")\n",
    "from utils.dataclass import (\n",
    "    flatten_dict,\n",
    ")\n",
    "from volume.human_speech import (\n",
    "    HIGH_FREQUENCY_SPEECH_THRESHOLD,\n",
    ")\n",
    "from configs.paths import (\n",
    "    PROCESSED_DUSHA_CROWD_TRAIN_DIR_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TEST_DIR_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TRAIN_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH,\n",
    "    PROCESSED_DUSHA_CROWD_TEST_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH,\n",
    ")\n",
    "from models.config import (\n",
    "    TORCH_TENSORS_KEYWOED,\n",
    "    ATTENTION_MASK_KEYWORD,\n",
    ")\n",
    "from models.text_embedding.ru_en_RoSBERTa import (\n",
    "    DEVICE as ROSBERTA_DEVICE,\n",
    "    NORMALIZE_P as ROSBERTA_NORMALIZE_P,\n",
    "    NORMALIZE_DIM as ROSBERTA_NORMALIZE_DIM,\n",
    "    CLAMP_MIN,\n",
    ")\n",
    "from config import (\n",
    "    SPEAKER_TEXT_FIELD_NAME,\n",
    ")\n",
    "from utils.parallel_processing import (\n",
    "    divide_into_chunks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whisper_audio_file_2_transcription(\n",
    "#     audio_path=Path('/data01/vvkiselev/data/other/dpl/dusha/crowd/crowd_train/wavs/000039c2bc753aa5a776621a4707eb73.wav'),\n",
    "#     processor=WHISPER_PROCESSOR,\n",
    "#     model=WHISPER_MODEL,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_AUDIO_PATH:Path = Path('/data01/vvkiselev/data/other/dpl/dusha/crowd/crowd_train/wavs/000039c2bc753aa5a776621a4707eb73.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Audio(sample_width=2, sr=16000, n_frames=165120, data=array([ 0,  0,  0, ..., -2,  6, -9], dtype=int16), n_channels=1, _transcription='ахах, пиздец')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_example:Audio = Audio.wav_file_path_init(path=EXAMPLE_AUDIO_PATH, transcription='ахах, пиздец')\n",
    "audio_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Audio(sample_width=2, sr=16000, n_frames=165120, data=array([-4, -3, -2, ...,  0, -1, -2], dtype=int16), n_channels=1, _transcription=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_filtered = HighLevelSpeechFeatures.speech_filter(audio=audio_example)\n",
    "\n",
    "HighLevelSpeechFeatures.speech_filter(audio=audio_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HighLevelSpeechFeatures(loudness=np.float64(60.43253714443894), HF_power_ratio=np.float64(0.05976286819121278), pronounce_speed=PronounceSpeed(WPS=1.065891472868217, LPS=0.9689922480620154, SPS=0.38759689922480617), transcription_features=TranscriptionHighLevelFeatures(mean_words_length=5.0, profanity_words_ratio=0.5))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HLF_example:HighLevelSpeechFeatures = HighLevelSpeechFeatures.wav_path_init(path=EXAMPLE_AUDIO_PATH, transcription='бля зачем')\n",
    "HLF_example:HighLevelSpeechFeatures = HighLevelSpeechFeatures.audio_init(audio=audio_example)\n",
    "HLF_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loudness': np.float64(60.43253714443894),\n",
       " 'HF_power_ratio': np.float64(0.05976286819121278),\n",
       " 'pronounce_speed_WPS': 0.7751937984496123,\n",
       " 'pronounce_speed_LPS': 0.7751937984496123,\n",
       " 'pronounce_speed_SPS': 0.29069767441860467,\n",
       " 'transcription_features_mean_words_length': 4.0,\n",
       " 'transcription_features_profanity_words_ratio': 0.5}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten_dict(asdict(HLF_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(906953, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_id</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>duration</th>\n",
       "      <th>annotator_emo</th>\n",
       "      <th>golden_emo</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>speaker_emo</th>\n",
       "      <th>source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475e76f77ac1ed7cabafca740b15b32a</td>\n",
       "      <td>wavs/475e76f77ac1ed7cabafca740b15b32a.wav</td>\n",
       "      <td>2.453000</td>\n",
       "      <td>angry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>не надо не надо не надо не надо</td>\n",
       "      <td>angry</td>\n",
       "      <td>fa136da095807ea6cd18dd6e2f58d4d0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f9438ef68395c70a8714dc373a49d11</td>\n",
       "      <td>wavs/2f9438ef68395c70a8714dc373a49d11.wav</td>\n",
       "      <td>4.640000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>фозил кори mp три</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3d436884cbbe25373914f8768de494f7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9937036a9c0dba20eecbffddd00f2be2</td>\n",
       "      <td>wavs/9937036a9c0dba20eecbffddd00f2be2.wav</td>\n",
       "      <td>4.341750</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2.0</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fb0ae78586a235018103acec22a80a8f</td>\n",
       "      <td>wavs/fb0ae78586a235018103acec22a80a8f.wav</td>\n",
       "      <td>3.900562</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>сколько стоит на керамбит</td>\n",
       "      <td>neutral</td>\n",
       "      <td>80bc833cf6b3f106d2e8991783a31e2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196dcf9e1aaac46c2aee45e7f6adfb92</td>\n",
       "      <td>wavs/196dcf9e1aaac46c2aee45e7f6adfb92.wav</td>\n",
       "      <td>4.780000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>афина когда закончится эта телепередача</td>\n",
       "      <td>neutral</td>\n",
       "      <td>bd78f079676fa5f1ed17253c9a440cc6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            hash_id  \\\n",
       "0  475e76f77ac1ed7cabafca740b15b32a   \n",
       "1  2f9438ef68395c70a8714dc373a49d11   \n",
       "2  9937036a9c0dba20eecbffddd00f2be2   \n",
       "3  fb0ae78586a235018103acec22a80a8f   \n",
       "4  196dcf9e1aaac46c2aee45e7f6adfb92   \n",
       "\n",
       "                                  audio_path  duration annotator_emo  \\\n",
       "0  wavs/475e76f77ac1ed7cabafca740b15b32a.wav  2.453000         angry   \n",
       "1  wavs/2f9438ef68395c70a8714dc373a49d11.wav  4.640000       neutral   \n",
       "2  wavs/9937036a9c0dba20eecbffddd00f2be2.wav  4.341750       neutral   \n",
       "3  wavs/fb0ae78586a235018103acec22a80a8f.wav  3.900562       neutral   \n",
       "4  wavs/196dcf9e1aaac46c2aee45e7f6adfb92.wav  4.780000       neutral   \n",
       "\n",
       "   golden_emo                      annotator_id  \\\n",
       "0         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "1         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "2         2.0  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "3         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "4         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "\n",
       "                              speaker_text speaker_emo  \\\n",
       "0          не надо не надо не надо не надо       angry   \n",
       "1                        фозил кори mp три     neutral   \n",
       "2                                      NaN         NaN   \n",
       "3                сколько стоит на керамбит     neutral   \n",
       "4  афина когда закончится эта телепередача     neutral   \n",
       "\n",
       "                          source_id  \n",
       "0  fa136da095807ea6cd18dd6e2f58d4d0  \n",
       "1  3d436884cbbe25373914f8768de494f7  \n",
       "2                               NaN  \n",
       "3  80bc833cf6b3f106d2e8991783a31e2b  \n",
       "4  bd78f079676fa5f1ed17253c9a440cc6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79088, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_id</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>duration</th>\n",
       "      <th>annotator_emo</th>\n",
       "      <th>golden_emo</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>speaker_text</th>\n",
       "      <th>speaker_emo</th>\n",
       "      <th>source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e9961c53ca6eeb440b217e539fbf46c</td>\n",
       "      <td>wavs/9e9961c53ca6eeb440b217e539fbf46c.wav</td>\n",
       "      <td>5.82</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>я слушаю</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4282ddc30d71ef420e202e0c60391e9f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0166f65a30354db8282682b1a280e64c</td>\n",
       "      <td>wavs/0166f65a30354db8282682b1a280e64c.wav</td>\n",
       "      <td>3.70</td>\n",
       "      <td>sad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>каким стал сбер</td>\n",
       "      <td>neutral</td>\n",
       "      <td>d70dc98ed56e9362eaefefb7b2827c8f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d49a6b560155831725a7bdc7d0a96099</td>\n",
       "      <td>wavs/d49a6b560155831725a7bdc7d0a96099.wav</td>\n",
       "      <td>4.38</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>где родился шерлок холмс</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0ee35d2abecf4272ecc8e1539b0839d8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c6852b0925797612d7b6724da8cbe7b4</td>\n",
       "      <td>wavs/c6852b0925797612d7b6724da8cbe7b4.wav</td>\n",
       "      <td>8.58</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>858305a5450b7bd1288ba0053b1cd1c1</td>\n",
       "      <td>открой в браузере ennio morricone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0855e363c1787df1592f58f7a27ebe13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0166f65a30354db8282682b1a280e64c</td>\n",
       "      <td>wavs/0166f65a30354db8282682b1a280e64c.wav</td>\n",
       "      <td>3.70</td>\n",
       "      <td>sad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a5562e26cd8f1949488a2d1e1e549d97</td>\n",
       "      <td>каким стал сбер</td>\n",
       "      <td>neutral</td>\n",
       "      <td>d70dc98ed56e9362eaefefb7b2827c8f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            hash_id  \\\n",
       "0  9e9961c53ca6eeb440b217e539fbf46c   \n",
       "1  0166f65a30354db8282682b1a280e64c   \n",
       "2  d49a6b560155831725a7bdc7d0a96099   \n",
       "3  c6852b0925797612d7b6724da8cbe7b4   \n",
       "4  0166f65a30354db8282682b1a280e64c   \n",
       "\n",
       "                                  audio_path  duration annotator_emo  \\\n",
       "0  wavs/9e9961c53ca6eeb440b217e539fbf46c.wav      5.82       neutral   \n",
       "1  wavs/0166f65a30354db8282682b1a280e64c.wav      3.70           sad   \n",
       "2  wavs/d49a6b560155831725a7bdc7d0a96099.wav      4.38       neutral   \n",
       "3  wavs/c6852b0925797612d7b6724da8cbe7b4.wav      8.58       neutral   \n",
       "4  wavs/0166f65a30354db8282682b1a280e64c.wav      3.70           sad   \n",
       "\n",
       "   golden_emo                      annotator_id  \\\n",
       "0         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "1         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "2         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "3         NaN  858305a5450b7bd1288ba0053b1cd1c1   \n",
       "4         NaN  a5562e26cd8f1949488a2d1e1e549d97   \n",
       "\n",
       "                        speaker_text speaker_emo  \\\n",
       "0                           я слушаю     neutral   \n",
       "1                    каким стал сбер     neutral   \n",
       "2           где родился шерлок холмс     neutral   \n",
       "3  открой в браузере ennio morricone     neutral   \n",
       "4                    каким стал сбер     neutral   \n",
       "\n",
       "                          source_id  \n",
       "0  4282ddc30d71ef420e202e0c60391e9f  \n",
       "1  d70dc98ed56e9362eaefefb7b2827c8f  \n",
       "2  0ee35d2abecf4272ecc8e1539b0839d8  \n",
       "3  0855e363c1787df1592f58f7a27ebe13  \n",
       "4  d70dc98ed56e9362eaefefb7b2827c8f  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_crowd_train = pd.read_csv(DUSHA_CROWD_TRAIN_FILE_PATH, sep=TAB)\n",
    "print(raw_crowd_train.shape)\n",
    "display(raw_crowd_train.head())\n",
    "\n",
    "raw_crowd_test = pd.read_csv(DUSHA_CROWD_TEST_FILE_PATH, sep=TAB)\n",
    "print(raw_crowd_test.shape)\n",
    "display(raw_crowd_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hash_id                   475e76f77ac1ed7cabafca740b15b32a\n",
       "audio_path       wavs/475e76f77ac1ed7cabafca740b15b32a.wav\n",
       "duration                                             2.453\n",
       "annotator_emo                                        angry\n",
       "golden_emo                                             NaN\n",
       "annotator_id              1bdb1b94789f364f9b5521652a70fe30\n",
       "speaker_text               не надо не надо не надо не надо\n",
       "speaker_emo                                          angry\n",
       "source_id                 fa136da095807ea6cd18dd6e2f58d4d0\n",
       "Name: 102, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_crowd_train[raw_crowd_train.hash_id == '475e76f77ac1ed7cabafca740b15b32a'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_crowd_train.hash_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(906953, 9)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_crowd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184633, 17217)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_crowd_train.hash_id.unique()), len(raw_crowd_test.hash_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 37315.87it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 33394.14it/s]\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.58s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    HashHLF(hash='475e76f77ac1ed7cabafca740b15b32a...\n",
       "1    HashHLF(hash='2f9438ef68395c70a8714dc373a49d11...\n",
       "2    HashHLF(hash='9937036a9c0dba20eecbffddd00f2be2...\n",
       "3    HashHLF(hash='fb0ae78586a235018103acec22a80a8f...\n",
       "4    HashHLF(hash='196dcf9e1aaac46c2aee45e7f6adfb92...\n",
       "dtype: object"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_crowd:pd.DataFrame = raw_crowd_train\n",
    "HLF_series:pd.Series = raw_crowd_2_HLF(\n",
    "    df=raw_crowd,\n",
    "    wavs_dir_path=DUSHA_CROWD_TRAIN_WAVS_DIR_PATH,\n",
    "    num_processes=20,\n",
    "    chunks_quantity=2,\n",
    "    rows_quantity=10,\n",
    "    output_file_path=PROCESSED_DUSHA_CROWD_TRAIN_HLF_LAST_VERSION_FILE_PATH,\n",
    ")\n",
    "HLF_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "# # Initialize model and tokenizer\n",
    "# ROSBERTA_EMBEDDER_MODEL_NAME:str = 'ai-forever/ru-en-RoSBERTa'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(ROSBERTA_EMBEDDER_MODEL_NAME)\n",
    "# model = AutoModel.from_pretrained(ROSBERTA_EMBEDDER_MODEL_NAME)\n",
    "\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output.last_hidden_state\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# # Russian text processing\n",
    "# texts = [\"Ваш текст на русском языке здесь\"]\n",
    "# encoded_input = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "\n",
    "# # Choose pooling method\n",
    "# embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "# # embeddings = model_output.last_hidden_state[:,0]  # CLS pooling alternative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(\n",
    "    model_output:transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions, \n",
    "    attention_mask:torch.Tensor,\n",
    "    )->torch.Tensor:\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=CLAMP_MIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at /data01/vvkiselev/data/other/dpl/models/ru-en-RoSBERTa and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path:Path = Path('/data01/vvkiselev/data/other/dpl/models/ru-en-RoSBERTa')\n",
    "tokenizer:transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast = AutoTokenizer.from_pretrained(model_path)\n",
    "model:transformers.models.roberta.modeling_roberta.RobertaModel = AutoModel.from_pretrained(model_path).to(ROSBERTA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divide_into_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_2_embeddings(\n",
    "    texts:List[str],\n",
    "    tokenizer:transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
    "    model:transformers.models.roberta.modeling_roberta.RobertaModel,\n",
    "    padding:bool=True,\n",
    "    truncation:bool=False,\n",
    "    return_tensors=TORCH_TENSORS_KEYWOED,\n",
    "    device=ROSBERTA_DEVICE,\n",
    "    attention_mask_keyword:str=ATTENTION_MASK_KEYWORD,\n",
    "    normalize_p:int = ROSBERTA_NORMALIZE_P,\n",
    "    normalize_dim:int = ROSBERTA_NORMALIZE_DIM,\n",
    "    )->torch.Tensor:\n",
    "    inputs:transformers.tokenization_utils_base.BatchEncoding = tokenizer(\n",
    "        texts,\n",
    "        padding=padding,\n",
    "        truncation=truncation,\n",
    "        return_tensors=return_tensors,\n",
    "    ).to(device)  # Move inputs to GPU\n",
    "\n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs:transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions = model(**inputs)\n",
    "\n",
    "    # Apply manual pooling\n",
    "    sentence_embeddings:torch.Tensor = mean_pooling(\n",
    "        model_output=outputs, \n",
    "        attention_mask=inputs[attention_mask_keyword],\n",
    "    )\n",
    "    embeddings:torch.Tensor = torch.nn.functional.normalize(\n",
    "        sentence_embeddings, \n",
    "        p=normalize_p, \n",
    "        dim=normalize_dim,\n",
    "    )\n",
    "\n",
    "    # print(f\"Embedding shape: {embeddings.shape}\")  # Output: torch.Size([1, 1024])\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2112, 1024])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs:torch.Tensor = texts_2_embeddings(\n",
    "    # texts=['Пример русского текста для анализа','я пошел гулять'],\n",
    "    texts=list(filter(lambda x: isinstance(x, str), list(raw_crowd_train[SPEAKER_TEXT_FIELD_NAME].head(5000).unique()))),\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['не надо не надо не надо не надо',\n",
       " 'фозил кори mp три',\n",
       " nan,\n",
       " 'сколько стоит на керамбит',\n",
       " 'афина когда закончится эта телепередача']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_crowd_train.head()[SPEAKER_TEXT_FIELD_NAME].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcriptions_series_to_text_2_emb(\n",
    "    transcriptions_series:pd.Series,\n",
    "    tokenizer:transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast,\n",
    "    model:transformers.models.roberta.modeling_roberta.RobertaModel,\n",
    "    num_chunks:int,\n",
    "    padding:bool=True,\n",
    "    truncation:bool=False,\n",
    "    return_tensors=TORCH_TENSORS_KEYWOED,\n",
    "    device=ROSBERTA_DEVICE,\n",
    "    attention_mask_keyword:str=ATTENTION_MASK_KEYWORD,\n",
    "    normalize_p:int = ROSBERTA_NORMALIZE_P,\n",
    "    normalize_dim:int = ROSBERTA_NORMALIZE_DIM,\n",
    "    )->Dict[str, torch.Tensor]:\n",
    "    unique_texts:List[str] = list(\n",
    "        filter(\n",
    "            lambda x: isinstance(x, str), \n",
    "            list(transcriptions_series.unique())\n",
    "        )\n",
    "    )\n",
    "    print(f'len(unique_texts) = {len(unique_texts)}')\n",
    "\n",
    "    chunks:List[List[str]] = divide_into_chunks(unique_texts, num_chunks)\n",
    "    unique_text_2_embedding:Dict[str, torch.Tensor] = {}\n",
    "    for chunk in tqdm(chunks):\n",
    "        chunk_embeddings:torch.Tensor = texts_2_embeddings(\n",
    "            texts=chunk,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            return_tensors=return_tensors,\n",
    "            device=device,\n",
    "            attention_mask_keyword=attention_mask_keyword,\n",
    "            normalize_p=normalize_p,\n",
    "            normalize_dim=normalize_dim,\n",
    "        ).cpu()\n",
    "        # print(f'chunk_embeddings.shape = {chunk_embeddings.shape}')\n",
    "        for chunk_i in range(len(chunk)):\n",
    "            unique_text_2_embedding[chunk[chunk_i]] = chunk_embeddings[chunk_i]\n",
    "\n",
    "    return unique_text_2_embedding\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(unique_texts) = 124568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:08<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "text_2_emb_train:Dict[str, torch.Tensor] = transcriptions_series_to_text_2_emb(\n",
    "    transcriptions_series=raw_crowd_train[SPEAKER_TEXT_FIELD_NAME],\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    num_chunks=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "не надо не надо не надо не надо                   tensor([ 0.0409,  0.0645, -0.0062,  ...,  0.01...\n",
       "фозил кори mp три                                 tensor([-0.0066,  0.0331,  0.0023,  ...,  0.01...\n",
       "сколько стоит на керамбит                         tensor([ 0.0246,  0.0035,  0.0029,  ..., -0.02...\n",
       "афина когда закончится эта телепередача           tensor([ 0.0242,  0.0469,  0.0188,  ...,  0.01...\n",
       "где проживают дети путина тихонова и воронцова    tensor([-0.0016, -0.0451, -0.0002,  ...,  0.05...\n",
       "dtype: object"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2_emb_series_train:pd.Series = pd.Series(index=text_2_emb_train.keys(), data=map(repr, text_2_emb_train.values()))\n",
    "text_2_emb_series_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_2_emb_train, PROCESSED_DUSHA_CROWD_TRAIN_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data01/vvkiselev/data/other/dpl/processed/dusha/crowd/train/text_embeddings/v1.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[332]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the dictionary from the file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m loaded_dict = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPROCESSED_DUSHA_CROWD_TRAIN_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mlist\u001b[39m(loaded_dict.items())[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m].shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/dpl/venv/lib64/python3.11/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/data01/vvkiselev/data/other/dpl/processed/dusha/crowd/train/text_embeddings/v1.pt'"
     ]
    }
   ],
   "source": [
    "# Load the dictionary from the file\n",
    "# loaded_dict = torch.load(PROCESSED_DUSHA_CROWD_TRAIN_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH, weights_only=False)\n",
    "# list(loaded_dict.items())[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(unique_texts) = 16628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.82it/s]\n"
     ]
    }
   ],
   "source": [
    "text_2_emb_test:Dict[str, torch.Tensor] = transcriptions_series_to_text_2_emb(\n",
    "    transcriptions_series=raw_crowd_test[SPEAKER_TEXT_FIELD_NAME],\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    num_chunks=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_2_emb_test, PROCESSED_DUSHA_CROWD_TEST_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dict = torch.load(PROCESSED_DUSHA_CROWD_TEST_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH, weights_only=False)\n",
    "list(loaded_dict.items())[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESSED_DUSHA_CROWD_TEST_TEXT_EMBEDDINGS_LAST_VERSION_FILE_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
